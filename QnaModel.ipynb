{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aO5dMtjdsur3"
      },
      "outputs": [],
      "source": [
        "!pip install pymysql\n",
        "!pip install --upgrade sagemaker\n",
        "!pip install urllib3 --upgrade\n",
        "!pip install boto3 --upgrade\n",
        "#!pip uninstall botocore\n",
        "#!pip install botocore\n",
        "!pip install botocore --upgrade\n",
        "\n",
        "#file name :QnaModel.py\n",
        "import pymysql \n",
        "import pandas as pd\n",
        "from os import path\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from DataFetchServices import *\n",
        "from modelServices import *\n",
        "from modelPredict import *\n",
        "class QnAModel():\n",
        "    \n",
        "    def __init__(self):\n",
        "        try:\n",
        "            os.mkdir('squad')\n",
        "        except:\n",
        "            print(\"Folder already exists\")\n",
        "    def fetch_data_from_db(self):\n",
        "        create_datasets()\n",
        "        \n",
        "\n",
        "    # prepare train and test data  to train bert  \n",
        "    def data_prep_for_model(self):\n",
        "        \n",
        "        def read_squad(path):\n",
        "            \n",
        "            with open(path,'rb') as f:\n",
        "                #load the train and test JSON file  into squad_dict \n",
        "                squad_dict = json.load(f)\n",
        "            # read the context , questions and answers from squad_dict and put them into contexts , questions and answers list objects respectivel \n",
        "            contexts = []\n",
        "            questions = []\n",
        "            answers = []\n",
        "            for group in squad_dict['data']:\n",
        "                for passage in group['paragraphs']:\n",
        "                    context = passage[\"context\"]\n",
        "                    for qa in passage['qas']:\n",
        "                        question = qa[\"question\"]\n",
        "                        for answer in qa[\"answers\"]:\n",
        "                            contexts.append(context)\n",
        "                            questions.append(question)\n",
        "                            answers.append(answer)\n",
        "            return {\"contexts\":contexts,\"questions\":questions,\"answers\":answers}\n",
        "\n",
        "        # create train_dataset and test_dataset  with \"contexts\":contexts,\"questions\":questions,\"answers\":answers\n",
        "        train_dataset = read_squad(\"./squad/training_data1.json\")\n",
        "        test_dataset = read_squad(\"./squad/test_data1.json\")\n",
        "\n",
        "\n",
        "        def add_end_index(answers,contexts):\n",
        "            # loop through each answers and contexts that you got in the read_squad function \n",
        "            for answer,context in zip(answers,contexts):\n",
        "                '''\n",
        "                \"answers\": [\n",
        "                                {\n",
        "                                    \"answer_start\": 42,\n",
        "                                    \"text\": \"senco\"\n",
        "                                }\n",
        "\n",
        "                '''\n",
        "                # put the  individual answer text( which is company name) in gold_text\n",
        "                gold_text = answer['text']\n",
        "                # put the data from answer_start into start_idx \n",
        "                start_idx = answer['answer_start']\n",
        "                # get the end_index for the text match( which is company name ) \n",
        "                end_idx = start_idx+len(gold_text)\n",
        "                # if the text between start and end index matches with text ( company name ), then put  end_index in answer_end\n",
        "                if context[start_idx:end_idx] == gold_text:\n",
        "                    answer['answer_end'] = end_idx\n",
        "                else:\n",
        "                    for n in [1,2]:\n",
        "                        if(context[start_idx-n:end_idx-n] == gold_text):\n",
        "                            answer['answer_end'] = end_idx-n\n",
        "                            answer['answer_start'] = start_idx-n\n",
        "\n",
        "        #add end_index to train and test dataset\n",
        "        add_end_index(train_dataset[\"answers\"],train_dataset[\"contexts\"])\n",
        "        add_end_index(test_dataset[\"answers\"],test_dataset[\"contexts\"])\n",
        "        return train_dataset,test_dataset\n",
        "\n",
        "    # end of data_prep_for_model\n",
        "\n",
        "\n",
        "    #fine tune the train data of bert\n",
        "      \n",
        "    def fine_tune_train(self,train_dataset,test_dataset,model_name='bert-base-uncased',tokenizer_name='bert-base-uncased',epochs=10,number_of_rows_data = 2000):\n",
        "        return fine_tune_qna_bert('bert-base-uncased','bert-base-uncased',epochs=3,train_dataset=train_dataset,test_dataset=test_dataset,number_of_rows_data = 2000)\n",
        "    \n",
        "    #predict the comany name from the daily IPO by reading  title and text  ( by comparing with the trained data taken from Multilex table )\n",
        "    #here we will usethe dataframe created out of the IPO files saved in S3 bucket \n",
        "    def predict_on_dataframe(self,input_dir, output_dir,tokenizer,device,myModel):\n",
        "        #return QnA(input_dir,output_dir,tokenizer,device,myModel)\n",
        "        #modified\n",
        "        # Bert based prediction complete \n",
        "        return QnA1(input_dir,output_dir,tokenizer,device,myModel)\n",
        "    \n",
        "    #save the pickle model \n",
        "    def save(self,model,model_path,model_name):\n",
        "        return pickle_save(model,model_path,model_name)\n",
        "    \n",
        "    #load the pickle model\n",
        "    def load(self,model_path,device):\n",
        "        with open(model_path, \"rb\") as newFile:\n",
        "            myModel = pickle.load(newFile)\n",
        "            myModel.to(device)\n",
        "        return myModel\n",
        "\n",
        "\n",
        "    def load_model_on_the_fly(self):\n",
        "        # call the create dataset function from Datafetchservices file where the data is trained from the multilex table using sklearn train and test model and fed into json file (both traing and test data ) \n",
        "        self.fetch_data_from_db()\n",
        "\n",
        "        #prepare the data model for bert\n",
        "        train_dataset,test_dataset = self.data_prep_for_model()\n",
        "\n",
        "        #Option 1 \n",
        "        # now train the bert model for making prediction \n",
        "\n",
        "        model,test_dataset_for_model,device,tokenizer = bert.fine_tune_train(train_dataset=train_dataset,test_dataset=test_dataset,model_name='bert-base-uncased',tokenizer_name='bert-base-uncased',epochs=1,number_of_rows_data = 2000)\n",
        "\n",
        "        #predict the company name from daily IPO xls\n",
        "\n",
        "        bert.predict_on_dataframe(\"\",\"\",tokenizer,device,model)\n",
        "\n",
        "\n",
        "    def use_pretrained_model(self):\n",
        "        # load the trained model from drive \n",
        "        from google.colab import drive\n",
        "        drive.mount(\"/content/drive\")\n",
        "\n",
        "        #!pip install transformers\n",
        "        import transformers\n",
        "\n",
        "        import pickle\n",
        "\n",
        "        #!ls '/content/drive'\n",
        "\n",
        "        # add shortcut of the pkl file in my drive to access via colab by right click on the pkl file and click on Add shortcut to Drive \n",
        "        # then copy it to the colab folder in the left side by the following command .y \n",
        "        !cp '/content/drive/My Drive/bert-base-uncased.pkl' bert-base-uncased.pkl\n",
        "        filename='bert-base-uncased.pkl'\n",
        "        #infile = open(filename,'rb')\n",
        "        #best_model2 = pickle.load(infile)\n",
        "       \n",
        "        \n",
        "\n",
        "        !pip install torch \n",
        "        !pip install tdqm \n",
        "        from transformers import BertTokenizerFast\n",
        "        from transformers import BertForQuestionAnswering\n",
        "        from torch.utils.data import DataLoader\n",
        "        from transformers import AdamW\n",
        "        from tqdm import tqdm \n",
        "        import torch\n",
        "\n",
        "\n",
        "        tokenizer_name='bert-base-uncased'\n",
        "        tokenizer = BertTokenizerFast.from_pretrained(tokenizer_name)\n",
        "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "        # using Vishwajeets function to load the model \n",
        "        model =self.load(filename,device)\n",
        "\n",
        "\n",
        "        bert.predict_on_dataframe(\"\",\"\",tokenizer,device,model)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # call the init function where it checks for the presence of squad dir else create it \n",
        "    bert = QnAModel()\n",
        "\n",
        "    #Option1 \n",
        "    #First train the model and then predict \n",
        "\n",
        "    #bert.load_model_on_the_fly()\n",
        "    \n",
        "    #Option 2 : \n",
        "    # use pretrained model and predict \n",
        "    bert.use_pretrained_model()\n",
        "\n",
        "        \n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5OmIBSn0K0B"
      },
      "outputs": [],
      "source": [
        "!pip install kora -q\n",
        "from kora import drive\n",
        "drive.link_nbs()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iNi57c_s3kLg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyNMQEBBJsUmqrJQ+yp7x3hR"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}