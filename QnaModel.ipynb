{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNz24qGCQPjSV3keCeO60PB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shashankgurunaga1/multilexQanA/blob/main/QnaModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oEPyy6Y4qjp",
        "outputId": "80f83bae-4650-46b7-eef4-5d5188ec3c4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (1.24.96)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from boto3) (0.6.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: botocore<1.28.0,>=1.27.96 in /usr/local/lib/python3.7/dist-packages (from boto3) (1.27.96)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.96->boto3) (1.26.12)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.96->boto3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.96->boto3) (1.15.0)\n",
            "Partiallycleanedipofile/PREIPO17thAug.xlsx\n",
            "download successful\n",
            "df1 8\n",
            "['Concord Biotech Ltd', 'PT Black Diamond Resources', 'Group One Holding', 'Digit Insurance', 'Kurly', 'The Flexi Group', 'Balaji Solutions', 'Starbox Group Holdings Ltd.']\n"
          ]
        }
      ],
      "source": [
        "!pip install boto3\n",
        "\n",
        "import pandas as pd\n",
        "import boto3\n",
        "import botocore.exceptions \n",
        "\n",
        "\n",
        "# read all the xlsx files from Partiallycleanedipofile\" and put the content in dataframe \n",
        "\n",
        "client = boto3.client('s3', aws_access_key_id = 'AKIAR7D36P44QPQK6IUY',\n",
        "    aws_secret_access_key = '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',\n",
        "    region_name = 'ap-south-1')\n",
        "\n",
        "bucket=\"multilex\"\n",
        "\n",
        "\n",
        "prefix=\"Partiallycleanedipofile\"\n",
        "\n",
        "response = client.list_objects_v2(\n",
        "        Bucket=bucket,\n",
        "        Prefix=prefix)\n",
        "\n",
        "df = pd.DataFrame()\n",
        "\n",
        "contents = []\n",
        "for content in response.get('Contents', []):\n",
        "  \n",
        "  list1=[]\n",
        "  list1=content['Key'].split(\"/\")\n",
        "  if len(list1[1]) != 0:\n",
        "      remotefilename=content['Key']\n",
        "      localfilename=list1[1]\n",
        "      print(remotefilename)\n",
        "      try:\n",
        "\n",
        "        client.download_file(bucket, remotefilename,localfilename)\n",
        "        print(\"download successful\")\n",
        "\n",
        "        df=df.append(pd.read_excel(localfilename))\n",
        "        \n",
        "        print(\"df1\" , df[df.columns[0]].count())\n",
        "\n",
        "      except botocore.exceptions.ClientError as e:\n",
        "        print(\"error in download\")\n",
        "\n",
        "#print(\"df2\", df)\n",
        "\n",
        "# read the company name \n",
        "\n",
        "company_list = df['Companies'].tolist()\n",
        "print(company_list)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WDdcGfgTqjI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i9-krRVvoVCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymysql\n",
        "!pip install json\n",
        "\n",
        "#file name : dataFetchServices.py \n",
        "import pymysql \n",
        "import pandas as pd\n",
        "from os import path\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def setup_connection():\n",
        "    user = 'admin'\n",
        "    DB_PASSWORD = 'HeyMultilex9087'\n",
        "    DB_PORT = 3306\n",
        "    passw = DB_PASSWORD\n",
        "    host =  'multilex-db.csgyi8splofr.ap-south-1.rds.amazonaws.com'\n",
        "    port = DB_PORT\n",
        "    database = 'preipo'\n",
        "    conn = pymysql.connect(host=host,port=port,user=user,passwd=passw,db=database)\n",
        "    return conn\n",
        "\n",
        "def fetch_data():\n",
        "    conn = setup_connection()\n",
        "    df = pd.read_sql('SELECT * FROM preipo.Multilex', con=conn)\n",
        "    return df\n",
        "def toJson(csvFilePath):\n",
        "    # read csv file and create a dataframe DF \n",
        "    Df = pd.read_csv(csvFilePath, na_filter=False, dtype=str)\n",
        "    # concatenate title and text of csv file  and out it in context in df\n",
        "    Df[\"context\"] = Df.title.str.cat(Df[\"text\"], sep=\" \")\n",
        "    # drop text and title from DF\n",
        "    Df.drop([\"text\", \"title\"], axis=1)\n",
        "    # keep only context and companies in Df \n",
        "    Df = Df[[\"context\",  \"Companies\"]]\n",
        "    # make everything in context in lower case \n",
        "    Df[\"context\"] = Df[\"context\"].apply(lambda x:str(x).lower())\n",
        "    # make everything in companies in lower case \n",
        "\n",
        "    Df[\"Companies\"] = Df[\"Companies\"].apply(lambda x:str(x).lower())\n",
        "    # parent_dict = {\"root\":}\n",
        "    data = []\n",
        "    # quetions whose answer to be trained using multilex table \n",
        "    questions = [\"which company is going ipo?\", \"which company is going to be listed?\",\n",
        "                 \"which company is going public?\"]\n",
        "    for index, row in Df.iterrows():\n",
        "         # create dictionary with key1- qas  as a iist and  context key as string \n",
        "        newDict = {\"qas\": [], \"context\": str(row[\"context\"])}\n",
        "        id = 1\n",
        "        for question in questions:\n",
        "             # for each question in questions create a tempDIct dctionary with\n",
        "             # key1- id , key2-is_impossible - question is impossible to get from context\n",
        "             # key3- quesiotn\n",
        "            tempDict = {\n",
        "                \"id\": str(id),\n",
        "                \"is_impossible\": False,\n",
        "                \"question\": question\n",
        "            }\n",
        "            # create a key4 -answers in tempDict \n",
        "            # answers is also a Dict with the below :\n",
        "                #key1 - text- take company name from df object based on multiulex table \n",
        "                # key2- answer_start - find the position of the company string  in the  text of context column in df which is created out of title and text of multilex table \n",
        "            tempDict[\"answers\"] = [\n",
        "                {\n",
        "                    \"text\": row[\"Companies\"],\n",
        "                    \"answer_start\": str(row[\"context\"]).lower().find(str(row[\"Companies\"]).lower())\n",
        "                }\n",
        "            ]\n",
        "            # if we get a match inside the context for the company name \n",
        "              # append the temDict object to newDict object created in the beginning \n",
        "              #increment the id \n",
        "            if str(row[\"context\"]).lower().find(str(row[\"Companies\"])) != -1:\n",
        "                newDict[\"qas\"].append(tempDict)\n",
        "                id += 1\n",
        "\n",
        "        '''\n",
        "        Example of data created using above logic \n",
        "        \"qas\": [\n",
        "                        {\n",
        "                            \"answers\": [\n",
        "                                {\n",
        "                                    \"answer_start\": 2574,\n",
        "                                    \"text\": \"donasi yang bisa\"\n",
        "                                }\n",
        "                            ],\n",
        "                            \"id\": \"1\",\n",
        "                            \"is_impossible\": false,\n",
        "                            \"question\": \"which company is going ipo?\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"answers\": [\n",
        "                                {\n",
        "                                    \"answer_start\": 2574,\n",
        "                                    \"text\": \"donasi yang bisa\"\n",
        "                                }\n",
        "                            ],\n",
        "                            \"id\": \"2\",\n",
        "                            \"is_impossible\": false,\n",
        "                            \"question\": \"which company is going to be listed?\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"answers\": [\n",
        "                                {\n",
        "                                    \"answer_start\": 2574,\n",
        "                                    \"text\": \"donasi yang bisa\"\n",
        "                                }\n",
        "                            ],\n",
        "                            \"id\": \"3\",\n",
        "                            \"is_impossible\": false,\n",
        "                            \"question\": \"which company is going public?\"\n",
        "                        }\n",
        "                    ]\n",
        "                },\n",
        "                {\n",
        "                    \"context\": \"want to ipo, estee gold feet euro market offer price rp 60 rp 70 per share reporter yuliana hema editor tendi mahadi kontan co id jakarta a prospective non cyclical issuer, pt estee gold feed tbk euro is ready to take the floor on the indonesia stock exchange through an initial public offering ipo the company, which is engaged in cosmetic aerosol filling services and household cleaning materials, offers a maximum of 500 million shares this value is equivalent to 20% of the company s issued and paid up capital after the ipo the shares are new shares issued from the portfolio with a nominal value of rp 5 per share of the total shares, estee gold feed offers an ipo price in the range of rp 60 to rp 70 per share thus, euro has the potential to reap fresh funds of up to rp 35 billion read also kusuma kemindo sentosa kkes targets ipo funds of up to idr 58 5 billion not only that, euro also issues a maximum of 500 series i warrants, equivalent to 25% of the total shares that have been assigned and fully paid these warrants will be given free of charge to shareholders whose names are recorded in the register of shareholders on the allotment date later, each holder of 1 one new share of estee gold feed will be entitled to 1 one series i warrant opex in its ipo celebration, estee gold feed appointed pt danatama makmur sekuritas to act as the underwriter next up facebook, whatsapp, instagram, netflix already registered pse, google, youtube haven t checked other news and articles on google news donate, get free vouchers your support will increase our enthusiasm to provide quality and useful articles as an expression of gratitude for your attention, there is a free voucher worth a donation that can be used for shopping at the kontan store support content news index tag ipo ipo plan ipo reporter yuliana hema editor tendi mahadi\",\n",
        "                      \n",
        "                      # second set for the second company \n",
        "                              \"qas\": [\n",
        "                        {\n",
        "                            \"answers\": [\n",
        "                                {\n",
        "                                    \"answer_start\": 173,\n",
        "                                    \"text\": \"pt estee gold feed tbk\"\n",
        "                                }\n",
        "                            ],\n",
        "                            \"id\": \"1\",\n",
        "                            \"is_impossible\": false,\n",
        "                            \"question\": \"which company is going ipo?\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"answers\": [\n",
        "                                {\n",
        "                                    \"answer_start\": 173,\n",
        "                                    \"text\": \"pt estee gold feed tbk\"\n",
        "                                }\n",
        "                            ],\n",
        "                            \"id\": \"2\",\n",
        "                            \"is_impossible\": false,\n",
        "                            \"question\": \"which company is going to be listed?\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"answers\": [\n",
        "                                {\n",
        "                                    \"answer_start\": 173,\n",
        "                                    \"text\": \"pt estee gold feed tbk\"\n",
        "                                }\n",
        "                            ],\n",
        "                            \"id\": \"3\",\n",
        "                            \"is_impossible\": false,\n",
        "                            \"question\": \"which company is going public?\"\n",
        "                        }\n",
        "                    ]\n",
        "                },\n",
        "                {\n",
        "                    \"context\": \"daftar ipo emiten 2022 targetnya ada 55 perusahaan bisnis com jakarta bursa efek indonesia menargetkan bakal ada 55 intial public offering ipo atau penawaran saham perdana dari emiten baru tahun ini siapakah mereka semua sampai dengan 14 januari 2022 bei telah mengantongi 30 nama perusahaan yang bakal tercatat tahun ini direktur penilaian perusahaan bei i gede nyoman yetna mengatakan dari 30 perusahaan tersebut empat perusahaan merupakan perusahaan beraset kecil dengan nilai aset di bawah rp50 miliar lalu 14 perusahaan merupakan perusahaan aset skala menengah dengan nilai aset antara rp50 miliar hingga rp250 miliar kemudian 12 perusahaan merupakan perusahaan beraset besar dengan nilai aset di atas rp250 miliar hingga saat ini terdapat 30 perusahaan dalam daftar antrian pipeline pencatatan saham bei sebagai informasi berikut rincian sektor calon emiten yang akan ipo tahun ini 4 perusahaan dari sektor industrials 4 perusahaan dari sektor consumer non cyclicals 9 perusahaan dari sektor consumer cyclicals 4 perusahaan dari sektor technology 1 perusahaan dari sektor healthcare 2 perusahaan dari sektor energy 1 perusahaan dari sektor financials 3 perusahaan dari sektor properties real estate 2 perusahaan dari sektor infrastructures sementara itu nyoman menambahkan telah tercatat dua perusahaan yang mencatatkan saham di bei dengan total dana yang berhasil dihimpun rp723 miliar yaitu pt adaro minerals indonesia tbk admr dan pt semacom integrated tbk sema selain kedua perusahaan masih ada beberapa emiten yang kini dalam penawaran misalnya pt champ resto indonesia tbk enak pt net visi media tbk netv pt nusatama berkah tbk ntbk dan pt mitra angkasa sejahtera tbk baut di sisi lain otoritas jasa keuangan ojk menargetkan penghimpunan dana di pasar modal mencapai rp125 triliun rp175 triliun pada 2022 target tersebut lebih rendah dibandingkan realisasi penghimpunan dana di pasar modal mencapai rp363 3 triliun pada 2021 berdasarkan data ojk penghimpunan dana di pasar modal terdiri dari penawaran umum perdana saham initial public offering ipo senilai rp61 7 triliun yang terdiri dari 56 emiten baru penerbitan hak dengan memesan efek terlebih dahulu rights issue senilai rp197 3 triliun dan penerbitan obligasi rp104 4 triliun adapun dana yang dihimpun lewat pasar modal pada tahun lalu melonjak 206 persen dibandingkan 2020 simak video pilihan di bawah ini\",\n",
        "\n",
        "        '''\n",
        "        # add newDict into data\n",
        "        data.append(newDict)\n",
        "    para_dict = {\"title\":\"pre ipo data\",\"paragraphs\":data}\n",
        "    data_dict = {\"data\":[para_dict]}\n",
        "    final_data_for_model = {\"root\":data_dict}\n",
        "    #print (final_data_for_model)\n",
        "    \n",
        "    fileName = path.basename(csvFilePath).split(sep=\".\")[0] + \"_data1.json\"\n",
        "\n",
        "    B_DIR=\"squad\"\n",
        "    newFileName = os.path.join(B_DIR,fileName)\n",
        "    \n",
        "    fileName = path.basename(csvFilePath).split(sep=\".\")[0] + \"_data1.json\"\n",
        "    with open(fileName, \"w\") as td:\n",
        "        json.dump(data_dict, td, indent=4, sort_keys=True)\n",
        "        print(\"dump successful\")\n",
        "\n",
        "\n",
        "# fetch data from Multilex Table \n",
        "# put it to training.csv and test.csv ( 80:20 ratio) using train anf test model from sklearn in machine learning \n",
        "# convert the CSV to JSON - \n",
        "def create_datasets():\n",
        "    df = fetch_data()\n",
        "    train,test = train_test_split(df,test_size=0.2)\n",
        "    train.to_csv(\"training.csv\")\n",
        "    test.to_csv(\"test.csv\")\n",
        "    #print(df.head())\n",
        "    toJson(\"training.csv\")\n",
        "    toJson(\"test.csv\")\n",
        "    \n",
        "    \n",
        "create_datasets()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yem4MNeYmr2I",
        "outputId": "f08118df-2899-4c78-d7b7-f51596d2cc81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pymysql in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement json (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for json\u001b[0m\n",
            "dump successful\n",
            "dump successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "maTwr0_12NmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load and save BERT Model  using pickle \n",
        "# concept - transformers, torch, tensor , token,encoding \n",
        "\n",
        "#file name : modelServices.py\n",
        "!pip install transformers\n",
        "!pip install torch \n",
        "!pip install tdqm \n",
        "\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers import BertForQuestionAnswering\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "from tqdm import tqdm \n",
        "import torch \n",
        "class SquadDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,encodings):\n",
        "    self.encodings = encodings\n",
        "  def __getitem__(self,idx):\n",
        "     return {key:torch.tensor(val[idx]) for key,val in self.encodings.items()}\n",
        "  def __len__(self):\n",
        "    return len(self.encodings.input_ids)\n",
        "\n",
        "\n",
        "def add_token_positions(encodings,answers,tokenizer,train_dataset):\n",
        "  start_positions = []\n",
        "  end_positions = []\n",
        "  for i in range(len(answers)):\n",
        "    start_positions.append(encodings.char_to_token(i,train_dataset[\"answers\"][i]['answer_start']))\n",
        "    end_positions.append(encodings.char_to_token(i,train_dataset[\"answers\"][i]['answer_end']))\n",
        "    if start_positions[-1] is None:\n",
        "      start_positions[-1] = tokenizer.model_max_length\n",
        "    go_back = 1\n",
        "    while end_positions[-1] is None:\n",
        "      end_positions[-1] = encodings.char_to_token(i,train_dataset[\"answers\"][i]['answer_end']-go_back)\n",
        "      go_back += 1\n",
        "  encodings.update({'start_positions':start_positions,'end_positions':end_positions})  \n",
        "  return encodings\n",
        "\n",
        "\n",
        "def get_dataset(tr_enc,tst_enc):\n",
        "  train_dataset_for_model = SquadDataset(tr_enc)\n",
        "  test_dataset_for_model = SquadDataset(tst_enc)\n",
        "  return train_dataset_for_model,test_dataset_for_model\n",
        "\n",
        "def get_encodings(tokenizer_name,number_of_rows_data,train_dataset,test_dataset):\n",
        "  tokenizer = BertTokenizerFast.from_pretrained(tokenizer_name)\n",
        "  train_encodings = tokenizer(train_dataset[\"contexts\"][0:number_of_rows_data],train_dataset[\"questions\"][0:number_of_rows_data],truncation=True,padding=True)\n",
        "  test_encodings = tokenizer(test_dataset[\"contexts\"][0:number_of_rows_data],test_dataset[\"questions\"][0:number_of_rows_data],truncation=True,padding=True)\n",
        "  train_encodings = add_token_positions(train_encodings,train_dataset[\"answers\"][0:number_of_rows_data],tokenizer,train_dataset)\n",
        "  test_encodings = add_token_positions(test_encodings,test_dataset[\"answers\"][0:number_of_rows_data],tokenizer,train_dataset)\n",
        "  return train_encodings,test_encodings\n",
        "\n",
        "\n",
        "def fine_tune_qna_bert(model_name,tokenizer_name,epochs,train_dataset,test_dataset,number_of_rows_data):\n",
        "  train_encodings,test_encodings = get_encodings(tokenizer_name,number_of_rows_data,train_dataset,test_dataset)\n",
        "  train_dataset_for_model,test_dataset_for_model = get_dataset(train_encodings,test_encodings)\n",
        "  tokenizer = BertTokenizerFast.from_pretrained(tokenizer_name)\n",
        "  model = BertForQuestionAnswering.from_pretrained(model_name)\n",
        "  device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "  # model.eval()\n",
        "  optim = AdamW(model.parameters(),lr=05e-5)\n",
        "  train_loader = DataLoader(train_dataset_for_model,batch_size=8,shuffle = True)\n",
        "  for epoch in range(epochs):\n",
        "    loop = tqdm(train_loader)\n",
        "    for batch in loop:\n",
        "      optim.zero_grad()\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      start_positions = batch['start_positions'].to(device)\n",
        "      end_positions = batch['end_positions'].to(device)\n",
        "      \n",
        "      outputs = model(input_ids,attention_mask=attention_mask,start_positions=start_positions,end_positions=end_positions)\n",
        "      loss = outputs[0]\n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "      loop.set_description(f'Epoch: {epoch}')\n",
        "      loop.set_postfix(loss=loss.item())\n",
        "  # model_path = f\"model/{model_name}/{epochs}/{number_of_rows_data}\"\n",
        "  # model.save_pretrained(model_path)\n",
        "  # tokenizer.save_pretrained(model_path)\n",
        "  return model,test_dataset_for_model,device,tokenizer\n",
        "\n",
        "\n",
        "import pickle\n",
        "def pickle_save(model,model_path,model_name):\n",
        "  pickle.dump(model,open(model_path + f\"{model_name}.pkl\",'wb'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwIpJ-IXqoBb",
        "outputId": "b669c315-d735-43f0-daf0-655be1c44ad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tdqm in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tdqm) (4.64.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#file name :QnaModel.py\n",
        "import pymysql \n",
        "import pandas as pd\n",
        "from os import path\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from dataFetchServices import *\n",
        "from modelServices import *\n",
        "from modelPredict import *\n",
        "class QnAModel():\n",
        "    \n",
        "    def __init__(self):\n",
        "        try:\n",
        "            os.mkdir('squad')\n",
        "        except:\n",
        "            print(\"Folder already exists\")\n",
        "    def fetch_data_from_db(self):\n",
        "        create_datasets()\n",
        "    def data_prep_for_model(self):\n",
        "        def read_squad(path):\n",
        "            with open(path,'rb') as f:\n",
        "                squad_dict = json.load(f)\n",
        "            contexts = []\n",
        "            questions = []\n",
        "            answers = []\n",
        "            for group in squad_dict['data']:\n",
        "                for passage in group['paragraphs']:\n",
        "                    context = passage[\"context\"]\n",
        "                    for qa in passage['qas']:\n",
        "                        question = qa[\"question\"]\n",
        "                        for answer in qa[\"answers\"]:\n",
        "                            contexts.append(context)\n",
        "                            questions.append(question)\n",
        "                            answers.append(answer)\n",
        "            return {\"contexts\":contexts,\"questions\":questions,\"answers\":answers}\n",
        "        train_dataset = read_squad(\"./squad/training_data1.json\")\n",
        "        test_dataset = read_squad(\"./squad/test_data1.json\")\n",
        "        def add_end_index(answers,contexts):\n",
        "            for answer,context in zip(answers,contexts):\n",
        "                gold_text = answer['text']\n",
        "                start_idx = answer['answer_start']\n",
        "                end_idx = start_idx+len(gold_text)\n",
        "                if context[start_idx:end_idx] == gold_text:\n",
        "                    answer['answer_end'] = end_idx\n",
        "                else:\n",
        "                    for n in [1,2]:\n",
        "                        if(context[start_idx-n:end_idx-n] == gold_text):\n",
        "                            answer['answer_end'] = end_idx-n\n",
        "                            answer['answer_start'] = start_idx-n\n",
        "        add_end_index(train_dataset[\"answers\"],train_dataset[\"contexts\"])\n",
        "        add_end_index(test_dataset[\"answers\"],test_dataset[\"contexts\"])\n",
        "        return train_dataset,test_dataset\n",
        "    def fine_tune_train(self,train_dataset,test_dataset,model_name='bert-base-uncased',tokenizer_name='bert-base-uncased',epochs=10,number_of_rows_data = 2000):\n",
        "        return fine_tune_qna_bert('bert-base-uncased','bert-base-uncased',epochs=3,train_dataset=train_dataset,test_dataset=test_dataset,number_of_rows_data = 2000)\n",
        "    def predict_on_dataframe(self,input_dir, output_dir,tokenizer,device,myModel):\n",
        "        return QnA(input_dir,output_dir,tokenizer,device,myModel)\n",
        "    def save(self,model,model_path,model_name):\n",
        "        return pickle_save(model,model_path,model_name)\n",
        "    def load(self,model_path):\n",
        "        with open(model_path, \"rb\") as newFile:\n",
        "            myModel = pickle.load(newFile)\n",
        "            myModel.to(device)\n",
        "        return myModel\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    bert = QnAModel()\n",
        "    bert.fetch_data_from_db()\n",
        "    train_dataset,test_dataset = bert.data_prep_for_model()\n",
        "    model,test_dataset_for_model,device,tokenizer = bert.fine_tune_train(train_dataset=train_dataset,test_dataset=test_dataset,model_name='bert-base-uncased',tokenizer_name='bert-base-uncased',epochs=10,number_of_rows_data = 2000)\n",
        "    # bert.predict_on_dataframe(\"\",\"\",tokenizer,device,model)\n",
        "Footer\n"
      ],
      "metadata": {
        "id": "aO5dMtjdsur3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}