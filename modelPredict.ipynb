{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNQecHl9MPxoYHIZvIknVV9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HHUvon6yEqGN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57b25fa4-ed1c-44af-e8b4-65e90316b918"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.25.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 8.3 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.1 MB/s \n",
            "\u001b[?25hCollecting botocore<1.29.0,>=1.28.0\n",
            "  Downloading botocore-1.28.0-py3-none-any.whl (9.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.3 MB 58.2 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 94.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.29.0,>=1.28.0->boto3) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.29.0,>=1.28.0->boto3) (1.15.0)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.12 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.25.0 botocore-1.28.0 jmespath-1.0.1 s3transfer-0.6.0 urllib3-1.26.12\n"
          ]
        }
      ],
      "source": [
        "#file name : modelPredict.py \n",
        "\n",
        "!pip install boto3\n",
        "questions = [\"which company is going ipo?\",\"which compay is going to be listed?\"]\n",
        "# torch.cuda.empty_cache()\n",
        "# myModel.to(device)\n",
        "def question_answer(question, text,tokenizer,device,myModel):\n",
        "    input_ids = tokenizer.encode(question, text)\n",
        "    \n",
        "    #string version of tokenized ids\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    #segment IDs\n",
        "    #first occurence of [SEP] token\n",
        "    sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
        "    #number of tokens in segment A (question)\n",
        "    num_seg_a = sep_idx+1\n",
        "    #number of tokens in segment B (text)\n",
        "    num_seg_b = len(input_ids) - num_seg_a\n",
        "    \n",
        "    #list of 0s and 1s for segment embeddings\n",
        "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "    assert len(segment_ids) == len(input_ids)\n",
        "    # segment_ids.to(device)\n",
        "    token_type_ids = torch.tensor([segment_ids]).to(device)\n",
        "    #model output using input_ids and segment_ids\n",
        "    i_ids = torch.tensor([input_ids]).to(device)\n",
        "    # print(token_type_ids)\n",
        "    # print(i_ids)\n",
        "    output = myModel(i_ids, token_type_ids=token_type_ids)\n",
        "    \n",
        "    #reconstructing the answer\n",
        "    answer_start = torch.argmax(output.start_logits)\n",
        "    answer_end = torch.argmax(output.end_logits)\n",
        "    if answer_end >= answer_start:\n",
        "        answer = tokens[answer_start]\n",
        "        for i in range(answer_start+1, answer_end+1):\n",
        "            if tokens[i][0:2] == \"##\":\n",
        "                answer += tokens[i][2:]\n",
        "            else:\n",
        "                answer += \" \" + tokens[i]\n",
        "    else:\n",
        "      answer= \"[CLS]\"          \n",
        "    if answer.startswith(\"[CLS]\"):\n",
        "        answer = \"Unable to find the answer to your question.\"\n",
        "    \n",
        "    return answer.capitalize()\n",
        "\n",
        "def QnA1():\n",
        "      \n",
        "    import pandas as pd\n",
        "    import boto3\n",
        "    import botocore.exceptions \n",
        "\n",
        "\n",
        "    # The access information is hardcoded only for testing purpose . for final , .env based approach to be used \n",
        "\n",
        "\n",
        "    # read all the xlsx files from Partiallycleanedipofile\" and put the content in dataframe \n",
        "\n",
        "    client = boto3.client('s3', aws_access_key_id = 'AKIAR7D36P44QPQK6IUY',\n",
        "        aws_secret_access_key = '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',\n",
        "        region_name = 'ap-south-1')\n",
        "\n",
        "    bucket=\"multilex\"\n",
        "\n",
        "\n",
        "    prefix=\"Uncleanedipofile\"\n",
        "\n",
        "    response = client.list_objects_v2(\n",
        "            Bucket=bucket,\n",
        "            Prefix=prefix)\n",
        "\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "    contents = []\n",
        "    for content in response.get('Contents', []):\n",
        "      \n",
        "      list1=[]\n",
        "      list1=content['Key'].split(\"/\")\n",
        "      if len(list1[1]) != 0:\n",
        "          remotefilename=content['Key']\n",
        "          localfilename=list1[1]\n",
        "          print(remotefilename)\n",
        "          try:\n",
        "\n",
        "            client.download_file(bucket, remotefilename,localfilename)\n",
        "            print(\"download successful\")\n",
        "\n",
        "            df=df.append(pd.read_csv(localfilename))\n",
        "            \n",
        "            print(\"df\" , df[df.columns[0]].count())\n",
        "\n",
        "          except botocore.exceptions.ClientError as e:\n",
        "            print(\"error in download\")\n",
        "\n",
        "\n",
        "    data = df[['text','publish_date','scraped_date','title','link']]\n",
        "    text = data['text']\n",
        "    title = data['title']\n",
        "    \n",
        "    company_names = []\n",
        "    for val,i in data.iterrows():\n",
        "        t = str(i[\"title\"]) + \" \" + str(i[\"text\"])\n",
        "        # print(t)\n",
        "        try:\n",
        "          c = question_answer(questions[0],t,tokenizer,device,myModel)\n",
        "          print(c)\n",
        "          company_names.append(question_answer(questions[0],t,tokenizer,device,myModel))\n",
        "        except:\n",
        "          t = t[0:512]\n",
        "          c = question_answer(questions[0],t,tokenizer,device,myModel)\n",
        "          company_names.append(\"preprocessing required\")\n",
        "    # data[\"Companies\"] = company_names\n",
        "    dic = {'Companies':company_names}\n",
        "    #[\"abc\",\"def\"]\n",
        "    df1 = pd.DataFrame(dic)\n",
        "    #data=[\"text\",\"title\"]\n",
        "    dff = [data,df1]\n",
        "    \n",
        "    df_final = pd.concat(dff,axis=1)\n",
        "    df_final.to_csv('EDI_PREIPO_report.csv',index=False)\n",
        "    print(\"df_final\" , df_final)\n",
        "    \n",
        "    #edi_preipo_report_fname = os.path.join( output_dir, 'EDI_PREIPO_report.csv' )\n",
        "    # logging.info(\"writing output artifact \" + edi_preipo_report_fname + \" to \" + output_dir)\n",
        "    # df_final.to_csv(edi_preipo_report_fname,index=False)\n",
        "    # logging.info(\"completed writing output artifact \" + edi_preipo_report_fname + \" to \" + output_dir)\n",
        "    return df_final\n",
        "\n",
        "def QnA(input_dir, output_dir,tokenizer,device,myModel):\n",
        "        import os\n",
        "        print(os.getcwd())\n",
        "        # %%\n",
        "        import pandas as pd\n",
        "        # import spacy\n",
        "        from datetime import datetime ,date\n",
        "        cur_date = str(date.today())\n",
        "        input_file_fullpath = os.path.join(input_dir,'todays_report.csv')\n",
        "        # logging.info(\"reading input artifact \" + input_file_fullpath)\n",
        "        data = pd.read_csv(input_file_fullpath, encoding='utf8')\n",
        "        # logging.info(\"completed reading input artifact \" + input_file_fullpath)\n",
        "        # data.drop('companies', inplace=True, axis=1)\n",
        "        data = data[['text','publish_date','scraped_date','title','link']]\n",
        "        text = data['text']\n",
        "        title = data['title']\n",
        "    \n",
        "        company_names = []\n",
        "        for val,i in data.iterrows():\n",
        "            t = str(i[\"title\"]) + \" \" + str(i[\"text\"])\n",
        "            # print(t)\n",
        "            try:\n",
        "              c = question_answer(questions[0],t,tokenizer,device,myModel)\n",
        "              print(c)\n",
        "              company_names.append(question_answer(questions[0],t,tokenizer,device,myModel))\n",
        "            except:\n",
        "              t = t[0:512]\n",
        "              c = question_answer(questions[0],t,tokenizer,device,myModel)\n",
        "              company_names.append(\"preprocessing required\")\n",
        "        # data[\"Companies\"] = company_names\n",
        "        dic = {'Companies':company_names}\n",
        "        #[\"abc\",\"def\"]\n",
        "        df = pd.DataFrame(dic)\n",
        "        #data=[\"text\",\"title\"]\n",
        "        dff = [data,df]\n",
        "        \n",
        "        df_final = pd.concat(dff,axis=1)\n",
        "        # df_final.to_csv('EDI_PREIPO_report.csv',index=False)\n",
        "        \n",
        "        edi_preipo_report_fname = os.path.join( output_dir, 'EDI_PREIPO_report.csv' )\n",
        "        # logging.info(\"writing output artifact \" + edi_preipo_report_fname + \" to \" + output_dir)\n",
        "        # df_final.to_csv(edi_preipo_report_fname,index=False)\n",
        "        # logging.info(\"completed writing output artifact \" + edi_preipo_report_fname + \" to \" + output_dir)\n",
        "        return df_final"
      ]
    }
  ]
}