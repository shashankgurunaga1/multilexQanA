{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZm00A2CNtmSM6qSRTjSeN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7NuRnLDx51y",
        "outputId": "461c872b-97a2-4047-e649-e2b06931b93a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 49.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 56.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.23.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tdqm\n",
            "  Downloading tdqm-0.0.1.tar.gz (1.4 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tdqm) (4.64.1)\n",
            "Building wheels for collected packages: tdqm\n",
            "  Building wheel for tdqm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tdqm: filename=tdqm-0.0.1-py3-none-any.whl size=1319 sha256=16e6b2162b6393bb20e2658b73716bcf5f5524c833449d00a3b6439685ce0895\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/f0/d9/9fa5ff78c0f9d5a0a427bbbb4893c283520ddfccb885ea2205\n",
            "Successfully built tdqm\n",
            "Installing collected packages: tdqm\n",
            "Successfully installed tdqm-0.0.1\n"
          ]
        }
      ],
      "source": [
        "# load and save BERT Model  using pickle \n",
        "# concept - transformers, torch, tensor , token,encoding \n",
        "\n",
        "#file name : modelServices.py\n",
        "!pip install transformers\n",
        "!pip install torch \n",
        "!pip install tdqm \n",
        "\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers import BertForQuestionAnswering\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "from tqdm import tqdm \n",
        "import torch \n",
        "class SquadDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,encodings):\n",
        "    self.encodings = encodings\n",
        "  def __getitem__(self,idx):\n",
        "     return {key:torch.tensor(val[idx]) for key,val in self.encodings.items()}\n",
        "  def __len__(self):\n",
        "    return len(self.encodings.input_ids)\n",
        "\n",
        "\n",
        "def add_token_positions(encodings,answers,tokenizer,train_dataset):\n",
        "  start_positions = []\n",
        "  end_positions = []\n",
        "  for i in range(len(answers)):\n",
        "    start_positions.append(encodings.char_to_token(i,train_dataset[\"answers\"][i]['answer_start']))\n",
        "    end_positions.append(encodings.char_to_token(i,train_dataset[\"answers\"][i]['answer_end']))\n",
        "    if start_positions[-1] is None:\n",
        "      start_positions[-1] = tokenizer.model_max_length\n",
        "    go_back = 1\n",
        "    while end_positions[-1] is None:\n",
        "      end_positions[-1] = encodings.char_to_token(i,train_dataset[\"answers\"][i]['answer_end']-go_back)\n",
        "      go_back += 1\n",
        "  encodings.update({'start_positions':start_positions,'end_positions':end_positions})  \n",
        "  return encodings\n",
        "\n",
        "\n",
        "def get_dataset(tr_enc,tst_enc):\n",
        "  train_dataset_for_model = SquadDataset(tr_enc)\n",
        "  test_dataset_for_model = SquadDataset(tst_enc)\n",
        "  return train_dataset_for_model,test_dataset_for_model\n",
        "\n",
        "def get_encodings(tokenizer_name,number_of_rows_data,train_dataset,test_dataset):\n",
        "  tokenizer = BertTokenizerFast.from_pretrained(tokenizer_name)\n",
        "  train_encodings = tokenizer(train_dataset[\"contexts\"][0:number_of_rows_data],train_dataset[\"questions\"][0:number_of_rows_data],truncation=True,padding=True)\n",
        "  test_encodings = tokenizer(test_dataset[\"contexts\"][0:number_of_rows_data],test_dataset[\"questions\"][0:number_of_rows_data],truncation=True,padding=True)\n",
        "  train_encodings = add_token_positions(train_encodings,train_dataset[\"answers\"][0:number_of_rows_data],tokenizer,train_dataset)\n",
        "  test_encodings = add_token_positions(test_encodings,test_dataset[\"answers\"][0:number_of_rows_data],tokenizer,train_dataset)\n",
        "  return train_encodings,test_encodings\n",
        "\n",
        "\n",
        "def fine_tune_qna_bert(model_name,tokenizer_name,epochs,train_dataset,test_dataset,number_of_rows_data):\n",
        "  train_encodings,test_encodings = get_encodings(tokenizer_name,number_of_rows_data,train_dataset,test_dataset)\n",
        "  train_dataset_for_model,test_dataset_for_model = get_dataset(train_encodings,test_encodings)\n",
        "  tokenizer = BertTokenizerFast.from_pretrained(tokenizer_name)\n",
        "  model = BertForQuestionAnswering.from_pretrained(model_name)\n",
        "  device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "  model.to(device)\n",
        "  model.train()\n",
        "  # model.eval()\n",
        "  optim = AdamW(model.parameters(),lr=05e-5)\n",
        "  train_loader = DataLoader(train_dataset_for_model,batch_size=8,shuffle = True)\n",
        "  for epoch in range(epochs):\n",
        "    loop = tqdm(train_loader)\n",
        "    for batch in loop:\n",
        "      optim.zero_grad()\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      start_positions = batch['start_positions'].to(device)\n",
        "      end_positions = batch['end_positions'].to(device)\n",
        "      \n",
        "      outputs = model(input_ids,attention_mask=attention_mask,start_positions=start_positions,end_positions=end_positions)\n",
        "      loss = outputs[0]\n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "      loop.set_description(f'Epoch: {epoch}')\n",
        "      loop.set_postfix(loss=loss.item())\n",
        "  # model_path = f\"model/{model_name}/{epochs}/{number_of_rows_data}\"\n",
        "  # model.save_pretrained(model_path)\n",
        "  # tokenizer.save_pretrained(model_path)\n",
        "  return model,test_dataset_for_model,device,tokenizer\n",
        "\n",
        "\n",
        "import pickle\n",
        "def pickle_save(model,model_path,model_name):\n",
        "  pickle.dump(model,open(model_path + f\"{model_name}.pkl\",'wb'))"
      ]
    }
  ]
}